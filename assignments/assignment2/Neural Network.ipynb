{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for linear1W\n",
      "Gradient check passed!\n",
      "Checking gradient for linear1B\n",
      "Gradient check passed!\n",
      "Checking gradient for linear2W\n",
      "Gradient check passed!\n",
      "Checking gradient for linear2B\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for linear1W\n",
      "Gradient check passed!\n",
      "Checking gradient for linear1B\n",
      "Gradient check passed!\n",
      "Checking gradient for linear2W\n",
      "Gradient check passed!\n",
      "Checking gradient for linear2B\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2] [9 9 2 1 7 1 3 5 3 6 4 2 9 1 0 7 2 1 4 7 5 9 5 3 8 9 6 6 5 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 1 1] [9 9 2 ... 4 2 7]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1] [2 9 8 4 6 5 3 3 3 1 1 7 5 2 8 8 3 3 2 3 3 7 3 1 4 8 1 7 0 1 8 8 4 6 1 1 0\n",
      " 6 1 5 1 1 1 5 1 3 5 2 8 2 7 1 7 4 3 3 6 5 7 3 2 3 1 5 1 2 3 4 2 3 3 2 1 8\n",
      " 3 1 1 5 5 0 9 6 2 0 9 3 1 1 3 4 1 1 3 2 2 4 7 6 6 1 9 7 3 7 1 4 9 0 1 1 6\n",
      " 7 1 7 6 5 1 5 1 1 4 4 1 3 6 5 4 1 1 5 2 1 2 1 7 7 5 1 1 1 1 9 7 3 2 8 6 2\n",
      " 1 2 2 9 2 1 6 0 4 0 1 3 0 9 8 3 3 1 2 8 2 1 3 6 2 4 1 2 4 8 5 2 1 9 5 3 1\n",
      " 4 8 5 1 7 7 3 7 4 1 1 4 5 5 2 5 1 1 2 3 3 5 1 9 7 2 4 1 7 4 3 2 2 6 2 0 6\n",
      " 1 6 5 5 8 2 5 1 1 1 2 0 6 1 5 6 1 1 8 8 3 4 5 2 7 1 8 1 2 9 3 9 1 8 0 9 3\n",
      " 7 1 9 2 5 7 7 1 3 2 1 4 2 0 0 3 4 4 3 6 8 3 3 1 5 7 0 9 1 1 1 9 3 7 1 2 8\n",
      " 1 1 2 7 8 1 4 5 3 3 7 9 1 9 7 8 1 3 2 1 6 4 3 2 1 1 6 1 1 1 1 1 9 6 2 2 2\n",
      " 2 7 4 2 7 9 5 2 3 1 9 4 0 1 8 7 5 7 2 7 3 3 6 2 2 6 6 1 6 5 3 1 1 2 0 3 8\n",
      " 4 9 3 4 7 8 0 5 4 7 6 1 3 4 5 9 1 9 8 2 5 0 3 6 1 2 3 1 6 2 4 3 9 3 0 9 1\n",
      " 6 4 6 3 4 8 1 9 6 9 6 5 1 6 3 1 3 0 5 2 6 9 3 8 4 2 1 7 7 0 7 5 2 4 3 6 1\n",
      " 2 8 1 5 4 7 1 0 3 3 1 7 4 3 1 7 1 5 0 3 5 4 2 2 4 0 3 1 3 2 7 3 6 6 1 1 3\n",
      " 1 3 4 2 5 3 1 2 4 1 1 4 5 1 9 9 2 6 4 2 5 6 3 2 3 4 5 0 2 2 1 2 3 4 4 1 9\n",
      " 1 1 3 6 2 2 8 6 1 4 1 3 3 1 2 2 5 1 3 3 5 1 3 6 3 5 2 2 3 4 4 7 7 4 5 3 4\n",
      " 2 1 7 7 0 7 9 9 1 5 8 7 6 4 9 4 1 1 6 2 6 1 5 8 2 6 5 2 2 6 5 6 6 3 0 1 3\n",
      " 7 5 0 4 2 3 1 8 5 1 3 3 5 9 1 3 2 9 5 4 8 1 8 2 8 1 9 2 1 2 1 9 6 4 4 1 0\n",
      " 2 2 4 1 1 1 8 6 3 7 2 1 1 8 4 6 2 4 0 7 3 1 8 8 1 2 7 2 3 1 3 7 1 4 2 5 6\n",
      " 4 7 9 8 6 2 2 8 2 3 1 5 0 8 8 1 3 1 5 1 1 0 8 6 3 3 0 2 6 4 6 5 3 7 1 2 4\n",
      " 6 5 6 7 5 6 2 3 3 0 9 0 3 8 3 6 1 7 4 6 3 9 3 2 4 3 3 0 5 3 2 2 4 1 3 6 2\n",
      " 1 9 6 3 1 7 3 2 4 4 7 2 2 6 3 5 6 1 5 5 7 2 1 3 9 1 5 2 6 9 5 8 4 1 7 1 5\n",
      " 2 4 4 3 1 9 3 9 8 3 1 3 0 8 4 2 3 8 3 4 1 9 1 3 3 4 4 1 7 5 0 3 7 4 5 9 0\n",
      " 5 4 2 4 3 2 3 1 7 5 2 1 0 7 6 4 6 7 5 7 7 2 1 2 2 1 0 3 9 1 5 1 4 1 5 5 3\n",
      " 0 1 1 0 3 1 1 9 2 1 5 3 2 7 1 1 3 3 1 1 4 3 3 7 9 7 2 7 5 3 8 6 3 0 6 2 8\n",
      " 6 4 3 1 8 1 0 1 1 4 1 1 0 3 3 6 4 8 5 0 4 8 2 4 1 2 2 1 6 1 3 9 1 5 2 1 3\n",
      " 5 7 1 8 6 1 5 5 0 3 4 2 2 2 1 9 5 1 9 3 8 2 2 1 9 2 6 4 1 8 6 2 3 1 5 2 9\n",
      " 8 1 3 7 1 2 9 2 4 2 2 0 8 9 2 5 1 2 1 9 7 4 3 1 1 2 3 2 8 4 7 3 1 5 2 5 3\n",
      " 3]\n",
      "Epoch: 0, Loss: 2.302029, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "[1 1 1 ... 1 1 1] [9 9 2 ... 4 2 7]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1] [2 9 8 4 6 5 3 3 3 1 1 7 5 2 8 8 3 3 2 3 3 7 3 1 4 8 1 7 0 1 8 8 4 6 1 1 0\n",
      " 6 1 5 1 1 1 5 1 3 5 2 8 2 7 1 7 4 3 3 6 5 7 3 2 3 1 5 1 2 3 4 2 3 3 2 1 8\n",
      " 3 1 1 5 5 0 9 6 2 0 9 3 1 1 3 4 1 1 3 2 2 4 7 6 6 1 9 7 3 7 1 4 9 0 1 1 6\n",
      " 7 1 7 6 5 1 5 1 1 4 4 1 3 6 5 4 1 1 5 2 1 2 1 7 7 5 1 1 1 1 9 7 3 2 8 6 2\n",
      " 1 2 2 9 2 1 6 0 4 0 1 3 0 9 8 3 3 1 2 8 2 1 3 6 2 4 1 2 4 8 5 2 1 9 5 3 1\n",
      " 4 8 5 1 7 7 3 7 4 1 1 4 5 5 2 5 1 1 2 3 3 5 1 9 7 2 4 1 7 4 3 2 2 6 2 0 6\n",
      " 1 6 5 5 8 2 5 1 1 1 2 0 6 1 5 6 1 1 8 8 3 4 5 2 7 1 8 1 2 9 3 9 1 8 0 9 3\n",
      " 7 1 9 2 5 7 7 1 3 2 1 4 2 0 0 3 4 4 3 6 8 3 3 1 5 7 0 9 1 1 1 9 3 7 1 2 8\n",
      " 1 1 2 7 8 1 4 5 3 3 7 9 1 9 7 8 1 3 2 1 6 4 3 2 1 1 6 1 1 1 1 1 9 6 2 2 2\n",
      " 2 7 4 2 7 9 5 2 3 1 9 4 0 1 8 7 5 7 2 7 3 3 6 2 2 6 6 1 6 5 3 1 1 2 0 3 8\n",
      " 4 9 3 4 7 8 0 5 4 7 6 1 3 4 5 9 1 9 8 2 5 0 3 6 1 2 3 1 6 2 4 3 9 3 0 9 1\n",
      " 6 4 6 3 4 8 1 9 6 9 6 5 1 6 3 1 3 0 5 2 6 9 3 8 4 2 1 7 7 0 7 5 2 4 3 6 1\n",
      " 2 8 1 5 4 7 1 0 3 3 1 7 4 3 1 7 1 5 0 3 5 4 2 2 4 0 3 1 3 2 7 3 6 6 1 1 3\n",
      " 1 3 4 2 5 3 1 2 4 1 1 4 5 1 9 9 2 6 4 2 5 6 3 2 3 4 5 0 2 2 1 2 3 4 4 1 9\n",
      " 1 1 3 6 2 2 8 6 1 4 1 3 3 1 2 2 5 1 3 3 5 1 3 6 3 5 2 2 3 4 4 7 7 4 5 3 4\n",
      " 2 1 7 7 0 7 9 9 1 5 8 7 6 4 9 4 1 1 6 2 6 1 5 8 2 6 5 2 2 6 5 6 6 3 0 1 3\n",
      " 7 5 0 4 2 3 1 8 5 1 3 3 5 9 1 3 2 9 5 4 8 1 8 2 8 1 9 2 1 2 1 9 6 4 4 1 0\n",
      " 2 2 4 1 1 1 8 6 3 7 2 1 1 8 4 6 2 4 0 7 3 1 8 8 1 2 7 2 3 1 3 7 1 4 2 5 6\n",
      " 4 7 9 8 6 2 2 8 2 3 1 5 0 8 8 1 3 1 5 1 1 0 8 6 3 3 0 2 6 4 6 5 3 7 1 2 4\n",
      " 6 5 6 7 5 6 2 3 3 0 9 0 3 8 3 6 1 7 4 6 3 9 3 2 4 3 3 0 5 3 2 2 4 1 3 6 2\n",
      " 1 9 6 3 1 7 3 2 4 4 7 2 2 6 3 5 6 1 5 5 7 2 1 3 9 1 5 2 6 9 5 8 4 1 7 1 5\n",
      " 2 4 4 3 1 9 3 9 8 3 1 3 0 8 4 2 3 8 3 4 1 9 1 3 3 4 4 1 7 5 0 3 7 4 5 9 0\n",
      " 5 4 2 4 3 2 3 1 7 5 2 1 0 7 6 4 6 7 5 7 7 2 1 2 2 1 0 3 9 1 5 1 4 1 5 5 3\n",
      " 0 1 1 0 3 1 1 9 2 1 5 3 2 7 1 1 3 3 1 1 4 3 3 7 9 7 2 7 5 3 8 6 3 0 6 2 8\n",
      " 6 4 3 1 8 1 0 1 1 4 1 1 0 3 3 6 4 8 5 0 4 8 2 4 1 2 2 1 6 1 3 9 1 5 2 1 3\n",
      " 5 7 1 8 6 1 5 5 0 3 4 2 2 2 1 9 5 1 9 3 8 2 2 1 9 2 6 4 1 8 6 2 3 1 5 2 9\n",
      " 8 1 3 7 1 2 9 2 4 2 2 0 8 9 2 5 1 2 1 9 7 4 3 1 1 2 3 2 8 4 7 3 1 5 2 5 3\n",
      " 3]\n",
      "Epoch: 1, Loss: 2.301685, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "[2 2 2 ... 2 2 2] [9 9 2 ... 4 2 7]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2] [2 9 8 4 6 5 3 3 3 1 1 7 5 2 8 8 3 3 2 3 3 7 3 1 4 8 1 7 0 1 8 8 4 6 1 1 0\n",
      " 6 1 5 1 1 1 5 1 3 5 2 8 2 7 1 7 4 3 3 6 5 7 3 2 3 1 5 1 2 3 4 2 3 3 2 1 8\n",
      " 3 1 1 5 5 0 9 6 2 0 9 3 1 1 3 4 1 1 3 2 2 4 7 6 6 1 9 7 3 7 1 4 9 0 1 1 6\n",
      " 7 1 7 6 5 1 5 1 1 4 4 1 3 6 5 4 1 1 5 2 1 2 1 7 7 5 1 1 1 1 9 7 3 2 8 6 2\n",
      " 1 2 2 9 2 1 6 0 4 0 1 3 0 9 8 3 3 1 2 8 2 1 3 6 2 4 1 2 4 8 5 2 1 9 5 3 1\n",
      " 4 8 5 1 7 7 3 7 4 1 1 4 5 5 2 5 1 1 2 3 3 5 1 9 7 2 4 1 7 4 3 2 2 6 2 0 6\n",
      " 1 6 5 5 8 2 5 1 1 1 2 0 6 1 5 6 1 1 8 8 3 4 5 2 7 1 8 1 2 9 3 9 1 8 0 9 3\n",
      " 7 1 9 2 5 7 7 1 3 2 1 4 2 0 0 3 4 4 3 6 8 3 3 1 5 7 0 9 1 1 1 9 3 7 1 2 8\n",
      " 1 1 2 7 8 1 4 5 3 3 7 9 1 9 7 8 1 3 2 1 6 4 3 2 1 1 6 1 1 1 1 1 9 6 2 2 2\n",
      " 2 7 4 2 7 9 5 2 3 1 9 4 0 1 8 7 5 7 2 7 3 3 6 2 2 6 6 1 6 5 3 1 1 2 0 3 8\n",
      " 4 9 3 4 7 8 0 5 4 7 6 1 3 4 5 9 1 9 8 2 5 0 3 6 1 2 3 1 6 2 4 3 9 3 0 9 1\n",
      " 6 4 6 3 4 8 1 9 6 9 6 5 1 6 3 1 3 0 5 2 6 9 3 8 4 2 1 7 7 0 7 5 2 4 3 6 1\n",
      " 2 8 1 5 4 7 1 0 3 3 1 7 4 3 1 7 1 5 0 3 5 4 2 2 4 0 3 1 3 2 7 3 6 6 1 1 3\n",
      " 1 3 4 2 5 3 1 2 4 1 1 4 5 1 9 9 2 6 4 2 5 6 3 2 3 4 5 0 2 2 1 2 3 4 4 1 9\n",
      " 1 1 3 6 2 2 8 6 1 4 1 3 3 1 2 2 5 1 3 3 5 1 3 6 3 5 2 2 3 4 4 7 7 4 5 3 4\n",
      " 2 1 7 7 0 7 9 9 1 5 8 7 6 4 9 4 1 1 6 2 6 1 5 8 2 6 5 2 2 6 5 6 6 3 0 1 3\n",
      " 7 5 0 4 2 3 1 8 5 1 3 3 5 9 1 3 2 9 5 4 8 1 8 2 8 1 9 2 1 2 1 9 6 4 4 1 0\n",
      " 2 2 4 1 1 1 8 6 3 7 2 1 1 8 4 6 2 4 0 7 3 1 8 8 1 2 7 2 3 1 3 7 1 4 2 5 6\n",
      " 4 7 9 8 6 2 2 8 2 3 1 5 0 8 8 1 3 1 5 1 1 0 8 6 3 3 0 2 6 4 6 5 3 7 1 2 4\n",
      " 6 5 6 7 5 6 2 3 3 0 9 0 3 8 3 6 1 7 4 6 3 9 3 2 4 3 3 0 5 3 2 2 4 1 3 6 2\n",
      " 1 9 6 3 1 7 3 2 4 4 7 2 2 6 3 5 6 1 5 5 7 2 1 3 9 1 5 2 6 9 5 8 4 1 7 1 5\n",
      " 2 4 4 3 1 9 3 9 8 3 1 3 0 8 4 2 3 8 3 4 1 9 1 3 3 4 4 1 7 5 0 3 7 4 5 9 0\n",
      " 5 4 2 4 3 2 3 1 7 5 2 1 0 7 6 4 6 7 5 7 7 2 1 2 2 1 0 3 9 1 5 1 4 1 5 5 3\n",
      " 0 1 1 0 3 1 1 9 2 1 5 3 2 7 1 1 3 3 1 1 4 3 3 7 9 7 2 7 5 3 8 6 3 0 6 2 8\n",
      " 6 4 3 1 8 1 0 1 1 4 1 1 0 3 3 6 4 8 5 0 4 8 2 4 1 2 2 1 6 1 3 9 1 5 2 1 3\n",
      " 5 7 1 8 6 1 5 5 0 3 4 2 2 2 1 9 5 1 9 3 8 2 2 1 9 2 6 4 1 8 6 2 3 1 5 2 9\n",
      " 8 1 3 7 1 2 9 2 4 2 2 0 8 9 2 5 1 2 1 9 7 4 3 1 1 2 3 2 8 4 7 3 1 5 2 5 3\n",
      " 3]\n",
      "Epoch: 2, Loss: 2.301851, Train accuracy: 0.148222, val accuracy: 0.140000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-2, num_epochs = 7)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 150, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, MomentumSGD(momentum = 0.99), learning_rate=0.22, learning_rate_decay=0.9, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = [0.25] #0.3\n",
    "reg_strength = 2e-3 #1e-3\n",
    "learning_rate_decay = 0.9 #0.6\n",
    "hidden_layer_size = 84 #128\n",
    "num_epochs = 22 #9\n",
    "batch_size = 301 #128\n",
    "momentum = 0.9 #0.9\n",
    "\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = hidden_layer_size, reg = reg_strength)\n",
    "    dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "    trainer = Trainer(model, dataset, MomentumSGD(momentum = momentum), learning_rate=learning_rate, learning_rate_decay=learning_rate_decay, num_epochs=num_epochs, batch_size=batch_size)\n",
    "\n",
    "    loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "    if np.max(val_history) > best_val_accuracy:\n",
    "        best_classifier = model\n",
    "        best_val_accuracy = np.max(val_history)\n",
    "        \n",
    "\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
