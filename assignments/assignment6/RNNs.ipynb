{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
    "\n",
    "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P59NYU98GCb9"
   },
   "outputs": [],
   "source": [
    "#!pip3 -qq install torch==0.4.1\n",
    "#!pip3 -qq install bokeh==0.13.0\n",
    "#!pip3 -qq install gensim==3.6.0\n",
    "#!pip3 -qq install nltk\n",
    "#!pip3 -qq install scikit-learn==0.20.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8sVtGHmA9aBM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6CNKM3b4hT1"
   },
   "source": [
    "# Рекуррентные нейронные сети (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O_XkoGNQUeGm"
   },
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFEtWrS_4rUs"
   },
   "source": [
    "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
    "\n",
    "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
    "\n",
    "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
    "\n",
    "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
    "\n",
    "Мы порешаем сейчас POS Tagging для английского.\n",
    "\n",
    "Будем работать с таким набором тегов:\n",
    "- ADJ - adjective (new, good, high, ...)\n",
    "- ADP - adposition (on, of, at, ...)\n",
    "- ADV - adverb (really, already, still, ...)\n",
    "- CONJ - conjunction (and, or, but, ...)\n",
    "- DET - determiner, article (the, a, some, ...)\n",
    "- NOUN - noun (year, home, costs, ...)\n",
    "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
    "- PRT - particle (at, on, out, ...)\n",
    "- PRON - pronoun (he, their, her, ...)\n",
    "- VERB - verb (is, say, told, ...)\n",
    "- . - punctuation marks (. , ;)\n",
    "- X - other (ersatz, esprit, dunno, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPIkKdFlHB-X"
   },
   "source": [
    "Скачаем данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TiA2dGmgF1rW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\mihan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\mihan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\universal_tagset.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d93g_swyJA_V"
   },
   "source": [
    "Пример размеченного предложения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QstS4NO0L97c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The            \tDET\n",
      "Fulton         \tNOUN\n",
      "County         \tNOUN\n",
      "Grand          \tADJ\n",
      "Jury           \tNOUN\n",
      "said           \tVERB\n",
      "Friday         \tNOUN\n",
      "an             \tDET\n",
      "investigation  \tNOUN\n",
      "of             \tADP\n",
      "Atlanta's      \tNOUN\n",
      "recent         \tADJ\n",
      "primary        \tNOUN\n",
      "election       \tNOUN\n",
      "produced       \tVERB\n",
      "``             \t.\n",
      "no             \tDET\n",
      "evidence       \tNOUN\n",
      "''             \t.\n",
      "that           \tADP\n",
      "any            \tDET\n",
      "irregularities \tNOUN\n",
      "took           \tVERB\n",
      "place          \tNOUN\n",
      ".              \t.\n"
     ]
    }
   ],
   "source": [
    "for word, tag in data[0]:\n",
    "    print('{:15}\\t{}'.format(word, tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epdW8u_YXcAv"
   },
   "source": [
    "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
    "\n",
    "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTai8Ta0lgwL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words count in train set: 739769\n",
      "Words count in val set: 130954\n",
      "Words count in test set: 290469\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
    "\n",
    "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
    "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
    "print('Words count in test set:', sum(len(sent) for sent in test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eChdLNGtXyP0"
   },
   "source": [
    "Построим маппинги из слов в индекс и из тега в индекс:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCjwwDs6Zq9x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in train = 45441. Tags = {'ADP', 'ADV', 'VERB', 'CONJ', 'X', 'DET', 'NUM', 'PRON', 'NOUN', 'PRT', 'ADJ', '.'}\n"
     ]
    }
   ],
   "source": [
    "words = {word for sample in train_data for word, tag in sample}\n",
    "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
    "word2ind['<pad>'] = 0\n",
    "\n",
    "tags = {tag for sample in train_data for word, tag in sample}\n",
    "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
    "tag2ind['<pad>'] = 0\n",
    "\n",
    "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "URC1B2nvPGFt"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
    "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "bar_width = 0.35\n",
    "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
    "plt.xticks(np.arange(len(tags)), tags)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gArQwbzWWkgi"
   },
   "source": [
    "## Бейзлайн\n",
    "\n",
    "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
    "\n",
    "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
    "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
    "\n",
    "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
    "\n",
    "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
    "\n",
    "Простейший вариант - униграммная модель, учитывающая только слово:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5rWmSToIaeAo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of unigram tagger = 92.62%\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "\n",
    "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
    "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07Ymb_MkbWsF"
   },
   "source": [
    "Добавим вероятности переходов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjz_Rk0bbMyH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of bigram tagger = 93.42%\n"
     ]
    }
   ],
   "source": [
    "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
    "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWMw6QHvbaDd"
   },
   "source": [
    "Обратите внимание, что `backoff` важен:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8XCuxEBVbOY_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of trigram tagger = 23.33%\n"
     ]
    }
   ],
   "source": [
    "trigram_tagger = nltk.TrigramTagger(train_data)\n",
    "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4t3xyYd__8d-"
   },
   "source": [
    "## Увеличиваем контекст с рекуррентными сетями\n",
    "\n",
    "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
    "\n",
    "Омонимия - основная причина, почему униграмная модель плоха:  \n",
    "*“he cashed a check at the **bank**”*  \n",
    "vs  \n",
    "*“he sat on the **bank** of the river”*\n",
    "\n",
    "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
    "\n",
    "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
    "\n",
    "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
    "\n",
    "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtRbz1SwgEqc"
   },
   "outputs": [],
   "source": [
    "def convert_data(data, word2ind, tag2ind):\n",
    "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
    "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
    "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
    "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhsTKZalfih6"
   },
   "outputs": [],
   "source": [
    "def iterate_batches(data, batch_size):\n",
    "    X, y = data\n",
    "    n_samples = len(X)\n",
    "\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        \n",
    "        batch_indices = indices[start:end]\n",
    "        \n",
    "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
    "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        \n",
    "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
    "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
    "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
    "            \n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4XsRII5kW5x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 4), (32, 4))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
    "\n",
    "X_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5I9E9P6eFYv"
   },
   "source": [
    "**Задание** Реализуйте `LSTMTagger`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WVEHju54d68T"
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        <create layers>\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        <apply them>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_HA8zyheYGH"
   },
   "source": [
    "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbrxsZ2mehWB"
   },
   "outputs": [],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ")\n",
    "\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "\n",
    "logits = model(X_batch)\n",
    "\n",
    "<calc accuracy>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GMUyUm1hgpe3"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "<calc loss>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSgV3NPUpcjH"
   },
   "source": [
    "**Задание** Вставьте эти вычисление в функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FprPQ0gllo7b"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    correct_count = 0\n",
    "    sum_count = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
    "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
    "                logits = model(X_batch)\n",
    "\n",
    "                loss = <calc loss>\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                cur_correct_count, cur_sum_count = <calc accuracy>\n",
    "\n",
    "                correct_count += cur_correct_count\n",
    "                sum_count += cur_sum_count\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
    "                )\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
    "            )\n",
    "\n",
    "    return epoch_loss / batches_count, correct_count / sum_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
    "        val_data=None, val_batch_size=None):\n",
    "        \n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_data is None:\n",
    "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pqfbeh1ltEYa"
   },
   "outputs": [],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0qGetIhfUE5"
   },
   "source": [
    "### Masking\n",
    "\n",
    "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
    "\n",
    "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAfV2dEOfHo5"
   },
   "source": [
    "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98wr38_rw55D"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXUTSFaEHbDG"
   },
   "source": [
    "### Bidirectional LSTM\n",
    "\n",
    "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
    "\n",
    "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
    "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
    "\n",
    "**Задание** Добавьте Bidirectional LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTXmYGD_ANhm"
   },
   "source": [
    "### Предобученные эмбеддинги\n",
    "\n",
    "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
    "\n",
    "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uZpY_Q1xZ18h"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "w2v_model = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYogOoKlgtcf"
   },
   "source": [
    "Построим подматрицу для слов из нашей тренировочной выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VsCstxiO03oT"
   },
   "outputs": [],
   "source": [
    "known_count = 0\n",
    "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
    "for word, ind in word2ind.items():\n",
    "    word = word.lower()\n",
    "    if word in w2v_model.vocab:\n",
    "        embeddings[ind] = w2v_model.get_vector(word)\n",
    "        known_count += 1\n",
    "        \n",
    "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcG7i-R8hbY3"
   },
   "source": [
    "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LxaRBpQd0pat"
   },
   "outputs": [],
   "source": [
    "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
    "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        <create me>\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        <use me>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EBtI6BDE-Fc7"
   },
   "outputs": [],
   "source": [
    "model = LSTMTaggerWithPretrainedEmbs(\n",
    "    embeddings=embeddings,\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Ne_8f24h8kg"
   },
   "source": [
    "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
    "\n",
    "Добейтесь качества лучше прошлых моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HPUuAPGhEGVR"
   },
   "outputs": [],
   "source": [
    "<calc test accuracy>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 06 - RNNs, part 2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
