{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.06106005e-09 4.53978686e-05 9.99954600e-01]\n",
      "[1. 0. 0.]\n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "print(probs)\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "print(probs)\n",
    "assert np.isclose(probs[0], 1.0)\n",
    "# Make sure it works on batches\n",
    "probs = linear_classifer.softmax(np.array([[1000, 0, 0], [2000, 0, 3000]]))\n",
    "print(probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), np.array([1]))\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, np.array([1])), np.array([1, 0, 0], np.float))\n",
    "\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([[1, 0, 0],[5,0,-5]]), np.array([1, 2]))\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, np.array([1, 2])), np.array([[1, 0, 0],[5,0,-5]], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "# print(W, W.shape,\"\\n---\\n\", X, X.shape, \"\\n---\\n\", target_index.shape, \"\\n---\\n\",)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.]\n",
      " [-1.  1.]\n",
      " [ 1.  2.]] 1.0877576813083574 [[ 0.28805844 -0.39402922  0.10597078]\n",
      " [ 0.49663118  0.00334627 -0.49997745]]\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "print(W,loss, grad)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X.shape\n",
      " (9000, 3073)\n",
      "train_y.shape\n",
      " (9000,)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "print(\"train_X.shape\\n\", train_X.shape)\n",
    "print(\"train_y.shape\\n\", train_y.shape)\n",
    "\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2a9bde4bf88>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.127\n",
      "Accuracy after training for 100 epochs:  0.121\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mihan\\py\\dlcourse_ai\\assignments\\assignment1\\linear_classifer.py:61: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = np.mean(-np.log(probs[np.arange(probs.shape[0]), target_index]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learining rate 0.99\n",
      "reg_strength 0.99\n",
      "accuracy 0.094\n",
      "learining rate 0.99\n",
      "reg_strength 0.1\n",
      "accuracy 0.169\n",
      "learining rate 0.99\n",
      "reg_strength 0.01\n",
      "accuracy 0.164\n",
      "learining rate 0.1\n",
      "reg_strength 0.99\n",
      "accuracy 0.166\n",
      "learining rate 0.1\n",
      "reg_strength 0.1\n",
      "accuracy 0.201\n",
      "learining rate 0.1\n",
      "reg_strength 0.01\n",
      "accuracy 0.241\n",
      "learining rate 0.01\n",
      "reg_strength 0.99\n",
      "accuracy 0.193\n",
      "learining rate 0.01\n",
      "reg_strength 0.1\n",
      "accuracy 0.23\n",
      "learining rate 0.01\n",
      "reg_strength 0.01\n",
      "accuracy 0.237\n",
      "best validation accuracy achieved: 0.241000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [0.99, 1e-1, 1e-2]\n",
    "reg_strengths = [0.99, 1e-1, 1e-2]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "\n",
    "for learining_rate in learning_rates:\n",
    "    for reg_strength in reg_strengths:\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "        classifier.fit(train_X, train_y, epochs=num_epochs, learning_rate=learining_rate, batch_size=batch_size, reg=reg_strength)\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        \n",
    "        print(\"learining rate\", learining_rate)\n",
    "        print(\"reg_strength\", reg_strength)\n",
    "        print(\"accuracy\", accuracy)\n",
    "        \n",
    "        if best_classifier is None:\n",
    "            best_classifier = classifier\n",
    "            best_val_accuracy = accuracy\n",
    "            \n",
    "        if best_val_accuracy < accuracy:\n",
    "            best_classifier = classifier\n",
    "            best_val_accuracy = accuracy\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.199000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
